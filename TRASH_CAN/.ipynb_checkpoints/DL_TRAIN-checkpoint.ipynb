{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34102b9-6c6a-4f30-b4de-669d3121c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 15:32:28.988369: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generated\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_training_unique_name() takes 10 positional arguments but 11 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17675/1136001708.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdata_unique_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtraining_unique_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunc_Train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_training_unique_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_unique_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_patience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_unique_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_unique_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_training_unique_name() takes 10 positional arguments but 11 were given"
     ]
    }
   ],
   "source": [
    "from py_env_train import *\n",
    "\n",
    "# Define the data specifications:\n",
    "model_data = [\"HRES\"]\n",
    "reference_data = [\"HSAF\"]\n",
    "task_name = \"spatiotemporal\"\n",
    "mm = \"MM\"  # or DM\n",
    "date_start=\"2020-07-01T13\"\n",
    "date_end=\"2023-03-26T23\"\n",
    "variable = \"pr\"\n",
    "mask_type = \"no_na\"\n",
    "laginensemble = False\n",
    "\n",
    "# Define the following for network configs:\n",
    "loss=\"mse\"\n",
    "Filters=32\n",
    "LR=0.01\n",
    "min_LR=0.0001\n",
    "BS=2\n",
    "lr_patience=2\n",
    "patience=8\n",
    "lr_factor=0.25\n",
    "epochs=64\n",
    "val_split=0.1\n",
    "n_channels=7\n",
    "xpixels=128\n",
    "ypixels=256\n",
    "\n",
    "######################################################################################################################################################\n",
    "\n",
    "filename = Func_Train.data_unique_name_generator(model_data, reference_data, task_name, mm, date_start, date_end, variable, mask_type, laginensemble)\n",
    "data_unique_name=filename[:-4]\n",
    "\n",
    "training_unique_name = Func_Train.generate_training_unique_name(data_unique_name, loss, Filters, LR, min_LR, lr_factor, lr_patience, BS, patience, val_split, epochs)\n",
    "\n",
    "print(data_unique_name, training_unique_name)\n",
    "\n",
    "# Create the training data (if doesn't exist)\n",
    "Func_Train.prepare_train(PPROJECT_DIR, TRAIN_FILES, ATMOS_DATA, filename, model_data, reference_data, task_name, mm, date_start, date_end, variable, mask_type, laginensemble, val_split, training_unique_name)\n",
    "\n",
    "# load the training data\n",
    "print(\"Loading training data...\")\n",
    "train_files=np.load(TRAIN_FILES+\"/\"+filename)\n",
    "\n",
    "train_x=train_files[\"train_x\"]\n",
    "train_y=train_files[\"train_y\"]\n",
    "train_m=train_files[\"train_m\"]\n",
    "val_x=train_files[\"val_x\"]\n",
    "val_y=train_files[\"val_y\"]\n",
    "val_m=train_files[\"val_m\"]\n",
    "\n",
    "model = Func_Train.UNET(xpixels, ypixels, n_channels, Filters)\n",
    "\n",
    "# Define optimizer and compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR, name='Adam')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['mse'])\n",
    "\n",
    "# Define the model checkpoint and early stopping callbacks\n",
    "model_path = PPROJECT_DIR+'/AI MODELS/00-UNET/'+training_unique_name+'.h5'\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(model_path, verbose=2, save_best_only=True, monitor='val_loss')\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=patience, monitor='val_loss'),\n",
    "             tf.keras.callbacks.TensorBoard(log_dir=PPROJECT_DIR+'/AI MODELS/00-UNET/'+training_unique_name)]\n",
    "\n",
    "# Define the ReduceLROnPlateau callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_factor, patience=lr_patience, min_lr=min_LR)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "# Train the model using train_x, train_y, train_m and val_x, val_y, val_m\n",
    "results = model.fit(train_x, train_y, validation_data=(val_x, val_y, val_m), \n",
    "                    batch_size=BS, epochs=epochs, verbose=1, \n",
    "                    callbacks=[callbacks, checkpointer, reduce_lr], sample_weight=train_m)\n",
    "# Save and plot the results\n",
    "print(\"Saving and plotting the results...\")\n",
    "RESULTS_DF=pd.DataFrame(results.history)\n",
    "RESULTS_DF.to_csv(DUMP_RESULTS+\"/\"+training_unique_name+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.1",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
